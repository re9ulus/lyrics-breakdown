{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.8075244024440723), (4, 0.5898341626740045)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],\n",
    "           [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)],\n",
    "           [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)],\n",
    "           [(0, 1.0), (4, 2.0), (7, 1.0)],\n",
    "           [(3, 1.0), (5, 1.0), (6, 1.0)],\n",
    "           [(9, 1.0)],\n",
    "           [(9, 1.0), (10, 1.0)],\n",
    "           [(9, 1.0), (10, 1.0), (11, 1.0)],\n",
    "           [(8, 1.0), (10, 1.0), (11, 1.0)]]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "vec = [(0, 1), (4, 1)]\n",
    "tfidf[vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'minors': 11, u'graph': 10, u'system': 6, u'trees': 9, u'eps': 8, u'computer': 1, u'survey': 5, u'user': 7, u'human': 2, u'time': 4, u'interface': 0, u'response': 3}\n",
      "[(1, 1), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "        for document in documents]\n",
    "\n",
    "# remove word that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "        for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)\n",
    "\n",
    "new_doc = 'Human computer interaction with human human human'\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(0, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(2, 1), (6, 2), (8, 1)],\n",
       " [(3, 1), (4, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(5, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BagWords\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('test.mm', corpus) # save to file for later use\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(1, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.44424552527467476), (6, 0.3244870206138555), (7, 0.3244870206138555)]\n",
      "[(0, 0.5710059809418182), (6, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(2, 0.49182558987264147), (6, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (4, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(5, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  u'-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.066007833960902804), (1, -0.52007033063618502)]\n",
      "[(0, 0.19667592859142299), (1, -0.76095631677000519)]\n",
      "[(0, 0.089926399724463077), (1, -0.72418606267525143)]\n",
      "[(0, 0.075858476521780557), (1, -0.63205515860034334)]\n",
      "[(0, 0.10150299184979941), (1, -0.57373084830029586)]\n",
      "[(0, 0.70321089393783165), (1, 0.16115180214025668)]\n",
      "[(0, 0.87747876731198393), (1, 0.16758906864659256)]\n",
      "[(0, 0.90986246868185872), (1, 0.14086553628718854)]\n",
      "[(0, 0.6165825350569285), (1, -0.053929075663894835)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lsi:\n",
    "    print(doc)\n",
    "# lsi.save('file_to_save.lsi')\n",
    "# lsi.load('file_to_load.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similiarity Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.079104751174447582), (1, -0.57328352430794027)]\n"
     ]
    }
   ],
   "source": [
    "doc = 'Human computer interaction'\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "print(vec_lsi) # vec_lsi order by similiarity to vec_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beatles test\n",
    "TODO: Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder = './../data/beatles/'\n",
    "num_topics = 5\n",
    "import random\n",
    "\n",
    "def read_data(folder):\n",
    "    texts, names = [], []\n",
    "    filenames = filter(lambda it: it.endswith('.txt'), os.listdir(folder))\n",
    "    for fname in filenames:\n",
    "        names.append(fname.replace('.txt', ''))\n",
    "        with open(folder + fname, 'r') as f:\n",
    "            texts.append(re.sub(r'\\[.*\\]', '', f.read().replace('\\n',' ').strip()))\n",
    "    return names, texts\n",
    "\n",
    "def prepare_data(records):\n",
    "    return [[word for word in record.translate(None, string.punctuation).lower().split()]\n",
    "        for record in records]\n",
    "\n",
    "def create_lsi(dictionary, corpus, num_topics=5):\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "    return lsi_model\n",
    "\n",
    "def cread_lda(dictionary, corpus, num_topics=5, passes=20):\n",
    "    number_of_topics = 5\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lda_model = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary,\n",
    "                                   num_topics=number_of_topics,\n",
    "                                   passes=passes)\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_names, raw_texts = read_data(folder)\n",
    "texts = prepare_data(raw_texts)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.234*\"love\" + 0.165*\"i\" + 0.139*\"im\" + 0.138*\"you\" + 0.135*\"ill\" + 0.129*\"me\" + 0.123*\"she\" + 0.119*\"yeah\" + 0.117*\"her\" + 0.117*\"be\"\n",
      "Topic 1 : -0.609*\"love\" + 0.231*\"back\" + 0.215*\"john\" + 0.154*\"brian\" + 0.120*\"get\" + -0.115*\"need\" + -0.113*\"true\" + 0.110*\"he\" + -0.109*\"ill\" + 0.108*\"paul\"\n",
      "Topic 2 : 0.328*\"she\" + -0.294*\"john\" + 0.290*\"her\" + -0.211*\"brian\" + 0.198*\"shes\" + -0.138*\"needed\" + 0.120*\"does\" + 0.120*\"down\" + -0.113*\"love\" + -0.113*\"paul\"\n",
      "Topic 3 : 0.241*\"she\" + 0.224*\"blackbird\" + 0.222*\"her\" + 0.185*\"john\" + 0.176*\"love\" + 0.165*\"moment\" + 0.158*\"fly\" + -0.151*\"baby\" + 0.150*\"girl\" + -0.144*\"want\"\n",
      "Topic 4 : 0.320*\"blackbird\" + -0.307*\"yeah\" + -0.263*\"love\" + -0.234*\"john\" + 0.231*\"fly\" + 0.230*\"moment\" + -0.203*\"brian\" + 0.198*\"waiting\" + 0.183*\"arise\" + 0.127*\"only\"\n"
     ]
    }
   ],
   "source": [
    "lsi_model = create_lsi(dictionary, corpus)\n",
    "for t, topic in lsi_model.print_topics(num_topics):\n",
    "    print('Topic {0} : {1}'.format(t, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Topic', 0, ':', u'0.002*i + 0.002*yeah + 0.002*long + 0.002*ah + 0.002*be')\n",
      "('Topic', 1, ':', u'0.002*christmas + 0.002*goodbye + 0.002*aw + 0.002*harp + 0.002*honey')\n",
      "('Topic', 2, ':', u'0.003*i + 0.002*what + 0.002*girl + 0.002*her + 0.002*get')\n",
      "('Topic', 3, ':', u'0.006*love + 0.003*dont + 0.003*you + 0.003*oh + 0.003*i')\n",
      "('Topic', 4, ':', u'0.003*baby + 0.003*cry + 0.002*no + 0.002*gonna + 0.002*im')\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "lda_model = cread_lda(dictionary, corpus, num_topics)\n",
    "for t, top_words in lda_model.print_topics(num_topics=num_topics, num_words=5):\n",
    "    print(\"Topic\", t, \":\", top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1822!', [(1, 0.52211132048712006), (3, 0.45661616145185091)])\n",
      "('A Day In The Life', [(0, 0.93234032464485683), (3, 0.064800838710947758)])\n",
      "(\"A Hard Day's Night\", [(2, 0.59639866382676765), (3, 0.401130343938853)])\n",
      "('A Little Rhyme', [(2, 0.84725434711504188), (3, 0.14571822990448782)])\n",
      "('A Shot Of Rhythm And Blues', [(2, 0.66083510545051505), (3, 0.33722872941850446)])\n",
      "('A Taste Of Honey', [(0, 0.99277053712726293)])\n",
      "('Across The Universe', [(2, 0.030486131292410516), (4, 0.96649820511462992)])\n",
      "('Act Naturally', [(3, 0.47476828978075786), (4, 0.52233039100101319)])\n",
      "(\"Ain't She Sweet\", [(2, 0.71058409092586106), (3, 0.28601270684145963)])\n",
      "(\"All I've Got To Do\", [(2, 0.24092343960557047), (3, 0.75527546340440466)])\n",
      "('All My Loving', [(3, 0.46507086697440275), (4, 0.53085947506280862)])\n",
      "('All Things Must Pass', [(3, 0.20633866267931289), (4, 0.78962031495826934)])\n",
      "('All Together Now', [(0, 0.99638830713218329)])\n",
      "('All You Need Is Love', [(3, 0.9964525033033127)])\n",
      "('And I Love Her', [(3, 0.99224448956609956)])\n",
      "('And Your Bird Can Sing', [(3, 0.99250958725230332)])\n",
      "('Anna, Go To Him', [(2, 0.774079799915664), (3, 0.21770531466863824)])\n",
      "('Another Girl', [(0, 0.67945646116787117), (2, 0.16519923469601974), (3, 0.15327811289958157)])\n",
      "('Any Time At All', [(3, 0.99480181265467194)])\n",
      "('Ask Me Why', [(2, 0.35004895702069394), (3, 0.64651732848793575)])\n"
     ]
    }
   ],
   "source": [
    "# Получение темы для конкретного документа\n",
    "for i in range(20):\n",
    "    print(song_names[i], lda_model[corpus[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
