{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\dev\\anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\soft\\dev\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import numpy as np\n",
    "\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 23:29:32,484 : INFO : collecting document frequencies\n",
      "2016-12-03 23:29:32,484 : INFO : PROGRESS: processing document #0\n",
      "2016-12-03 23:29:32,485 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.8075244024440723), (4, 0.5898341626740045)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],\n",
    "           [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)],\n",
    "           [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)],\n",
    "           [(0, 1.0), (4, 2.0), (7, 1.0)],\n",
    "           [(3, 1.0), (5, 1.0), (6, 1.0)],\n",
    "           [(9, 1.0)],\n",
    "           [(9, 1.0), (10, 1.0)],\n",
    "           [(9, 1.0), (10, 1.0), (11, 1.0)],\n",
    "           [(8, 1.0), (10, 1.0), (11, 1.0)]]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "vec = [(0, 1), (4, 1)]\n",
    "tfidf[vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 23:29:36,206 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2016-12-03 23:29:36,207 : INFO : built Dictionary(12 unique tokens: ['minors', 'trees', 'graph', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'minors': 11, 'trees': 9, 'graph': 10, 'response': 7, 'survey': 3, 'user': 6, 'computer': 2, 'human': 0, 'time': 4, 'system': 5, 'interface': 1, 'eps': 8}\n",
      "[(0, 4), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "        for document in documents]\n",
    "\n",
    "# remove word that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "        for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)\n",
    "\n",
    "new_doc = 'Human computer interaction with human human human'\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 23:29:38,735 : INFO : storing corpus in Matrix Market format to test.mm\n",
      "2016-12-03 23:29:38,735 : INFO : saving sparse matrix to test.mm\n",
      "2016-12-03 23:29:38,736 : INFO : PROGRESS: saving document #0\n",
      "2016-12-03 23:29:38,736 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2016-12-03 23:29:38,737 : INFO : saving MmCorpus index to test.mm.index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(1, 1), (5, 1), (6, 1), (8, 1)],\n",
       " [(0, 1), (5, 2), (8, 1)],\n",
       " [(4, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(3, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BagWords\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('test.mm', corpus) # save to file for later use\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(1, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.44424552527467476), (6, 0.3244870206138555), (7, 0.3244870206138555)]\n",
      "[(0, 0.5710059809418182), (6, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(2, 0.49182558987264147), (6, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (4, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(5, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  u'-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.066007833960902804), (1, -0.52007033063618502)]\n",
      "[(0, 0.19667592859142299), (1, -0.76095631677000519)]\n",
      "[(0, 0.089926399724463077), (1, -0.72418606267525143)]\n",
      "[(0, 0.075858476521780557), (1, -0.63205515860034334)]\n",
      "[(0, 0.10150299184979941), (1, -0.57373084830029586)]\n",
      "[(0, 0.70321089393783165), (1, 0.16115180214025668)]\n",
      "[(0, 0.87747876731198393), (1, 0.16758906864659256)]\n",
      "[(0, 0.90986246868185872), (1, 0.14086553628718854)]\n",
      "[(0, 0.6165825350569285), (1, -0.053929075663894835)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lsi:\n",
    "    print(doc)\n",
    "# lsi.save('file_to_save.lsi')\n",
    "# lsi.load('file_to_load.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similiarity Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.079104751174447582), (1, -0.57328352430794027)]\n"
     ]
    }
   ],
   "source": [
    "doc = 'Human computer interaction'\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "print(vec_lsi) # vec_lsi order by similiarity to vec_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beatles test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder = './../data/beatles/'\n",
    "num_topics = 5\n",
    "import random\n",
    "\n",
    "translator = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "def read_data(folder):\n",
    "    texts, names = [], []\n",
    "    filenames = filter(lambda it: it.endswith('.txt'), os.listdir(folder))\n",
    "    for fname in filenames:\n",
    "        names.append(fname.replace('.txt', ''))\n",
    "        with open(folder + fname, 'r') as f:\n",
    "            texts.append(re.sub(r'\\[.*\\]', '', f.read().replace('\\n',' ').strip()))\n",
    "    return names, texts\n",
    "\n",
    "def prepare_data(records):\n",
    "    return [[word for word in record.translate(translator).lower().split() if word not in STOPWORDS]\n",
    "        for record in records]\n",
    "\n",
    "def create_lsi(dictionary, corpus, num_topics=5):\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "    return lsi_model\n",
    "\n",
    "def cread_lda(dictionary, corpus, num_topics=5, passes=20, chunksize=2000):\n",
    "    number_of_topics = 5\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lda_model = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary,\n",
    "                                   num_topics=number_of_topics,\n",
    "                                   passes=passes,\n",
    "                                   chunksize=chunksize)\n",
    "    return lda_model\n",
    "\n",
    "def get_top_words(lda_model):\n",
    "    # http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\n",
    "    top_words = [[word for word, __ in lda_model.show_topic(topic, topn=50)] for topic in range(lda_model.num_topics)]\n",
    "    replacements = []\n",
    "    for words in top_words:\n",
    "        yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 23:28:14,893 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2016-12-03 23:28:14,906 : INFO : built Dictionary(2141 unique tokens: ['magical', 'believe', 'pop', 'bride', 'called']...) from 186 documents (total 14083 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "song_names, raw_texts = read_data(folder)\n",
    "texts = prepare_data(raw_texts)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 23:28:17,102 : INFO : collecting document frequencies\n",
      "2016-12-03 23:28:17,102 : INFO : PROGRESS: processing document #0\n",
      "2016-12-03 23:28:17,104 : INFO : calculating IDF weights for 186 documents and 2140 features (6169 matrix non-zeros)\n",
      "2016-12-03 23:28:17,106 : INFO : using serial LSI version on this node\n",
      "2016-12-03 23:28:17,107 : INFO : updating model with new documents\n",
      "2016-12-03 23:28:17,117 : INFO : preparing a new chunk of documents\n",
      "2016-12-03 23:28:17,143 : INFO : using 100 extra samples and 2 power iterations\n",
      "2016-12-03 23:28:17,143 : INFO : 1st phase: constructing (2141, 105) action matrix\n",
      "2016-12-03 23:28:17,166 : INFO : orthonormalizing (2141, 105) action matrix\n",
      "2016-12-03 23:28:17,482 : INFO : 2nd phase: running dense svd on (105, 186) matrix\n",
      "2016-12-03 23:28:17,604 : INFO : computing the final decomposition\n",
      "2016-12-03 23:28:17,604 : INFO : keeping 5 factors (discarding 87.926% of energy spectrum)\n",
      "2016-12-03 23:28:17,605 : INFO : processed documents up to #186\n",
      "2016-12-03 23:28:17,607 : INFO : topic #0(2.457): -0.402*\"love\" + -0.213*\"ill\" + -0.213*\"yeah\" + -0.207*\"im\" + -0.164*\"dont\" + -0.156*\"baby\" + -0.150*\"know\" + -0.144*\"girl\" + -0.143*\"want\" + -0.140*\"oh\"\n",
      "2016-12-03 23:28:17,607 : INFO : topic #1(1.701): -0.707*\"love\" + 0.168*\"yeah\" + 0.164*\"baby\" + 0.161*\"john\" + -0.133*\"need\" + -0.132*\"whoa\" + 0.119*\"im\" + -0.116*\"true\" + 0.116*\"got\" + 0.106*\"brian\"\n",
      "2016-12-03 23:28:17,608 : INFO : topic #2(1.537): 0.525*\"john\" + 0.391*\"brian\" + 0.220*\"paul\" + 0.184*\"love\" + 0.174*\"yeah\" + 0.151*\"beatles\" + -0.149*\"dont\" + 0.129*\"ha\" + 0.115*\"play\" + -0.108*\"let\"\n",
      "2016-12-03 23:28:17,609 : INFO : topic #3(1.499): 0.463*\"yeah\" + -0.345*\"girl\" + -0.228*\"blackbird\" + -0.197*\"moment\" + -0.161*\"fly\" + -0.154*\"john\" + -0.140*\"waiting\" + 0.138*\"ill\" + 0.137*\"gotta\" + -0.130*\"arise\"\n",
      "2016-12-03 23:28:17,609 : INFO : topic #4(1.481): 0.401*\"baby\" + -0.249*\"blackbird\" + -0.193*\"yeah\" + -0.182*\"fly\" + -0.172*\"waiting\" + -0.165*\"ill\" + 0.156*\"girl\" + -0.151*\"night\" + -0.150*\"moment\" + 0.149*\"man\"\n",
      "2016-12-03 23:28:17,610 : INFO : topic #0(2.457): -0.402*\"love\" + -0.213*\"ill\" + -0.213*\"yeah\" + -0.207*\"im\" + -0.164*\"dont\" + -0.156*\"baby\" + -0.150*\"know\" + -0.144*\"girl\" + -0.143*\"want\" + -0.140*\"oh\"\n",
      "2016-12-03 23:28:17,611 : INFO : topic #1(1.701): -0.707*\"love\" + 0.168*\"yeah\" + 0.164*\"baby\" + 0.161*\"john\" + -0.133*\"need\" + -0.132*\"whoa\" + 0.119*\"im\" + -0.116*\"true\" + 0.116*\"got\" + 0.106*\"brian\"\n",
      "2016-12-03 23:28:17,613 : INFO : topic #2(1.537): 0.525*\"john\" + 0.391*\"brian\" + 0.220*\"paul\" + 0.184*\"love\" + 0.174*\"yeah\" + 0.151*\"beatles\" + -0.149*\"dont\" + 0.129*\"ha\" + 0.115*\"play\" + -0.108*\"let\"\n",
      "2016-12-03 23:28:17,615 : INFO : topic #3(1.499): 0.463*\"yeah\" + -0.345*\"girl\" + -0.228*\"blackbird\" + -0.197*\"moment\" + -0.161*\"fly\" + -0.154*\"john\" + -0.140*\"waiting\" + 0.138*\"ill\" + 0.137*\"gotta\" + -0.130*\"arise\"\n",
      "2016-12-03 23:28:17,616 : INFO : topic #4(1.481): 0.401*\"baby\" + -0.249*\"blackbird\" + -0.193*\"yeah\" + -0.182*\"fly\" + -0.172*\"waiting\" + -0.165*\"ill\" + 0.156*\"girl\" + -0.151*\"night\" + -0.150*\"moment\" + 0.149*\"man\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : -0.402*\"love\" + -0.213*\"ill\" + -0.213*\"yeah\" + -0.207*\"im\" + -0.164*\"dont\" + -0.156*\"baby\" + -0.150*\"know\" + -0.144*\"girl\" + -0.143*\"want\" + -0.140*\"oh\"\n",
      "Topic 1 : -0.707*\"love\" + 0.168*\"yeah\" + 0.164*\"baby\" + 0.161*\"john\" + -0.133*\"need\" + -0.132*\"whoa\" + 0.119*\"im\" + -0.116*\"true\" + 0.116*\"got\" + 0.106*\"brian\"\n",
      "Topic 2 : 0.525*\"john\" + 0.391*\"brian\" + 0.220*\"paul\" + 0.184*\"love\" + 0.174*\"yeah\" + 0.151*\"beatles\" + -0.149*\"dont\" + 0.129*\"ha\" + 0.115*\"play\" + -0.108*\"let\"\n",
      "Topic 3 : 0.463*\"yeah\" + -0.345*\"girl\" + -0.228*\"blackbird\" + -0.197*\"moment\" + -0.161*\"fly\" + -0.154*\"john\" + -0.140*\"waiting\" + 0.138*\"ill\" + 0.137*\"gotta\" + -0.130*\"arise\"\n",
      "Topic 4 : 0.401*\"baby\" + -0.249*\"blackbird\" + -0.193*\"yeah\" + -0.182*\"fly\" + -0.172*\"waiting\" + -0.165*\"ill\" + 0.156*\"girl\" + -0.151*\"night\" + -0.150*\"moment\" + 0.149*\"man\"\n"
     ]
    }
   ],
   "source": [
    "lsi_model = create_lsi(dictionary, corpus)\n",
    "for t, topic in lsi_model.print_topics(num_topics):\n",
    "    print('Topic {0} : {1}'.format(t, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 23:28:21,566 : INFO : collecting document frequencies\n",
      "2016-12-03 23:28:21,566 : INFO : PROGRESS: processing document #0\n",
      "2016-12-03 23:28:21,568 : INFO : calculating IDF weights for 186 documents and 2140 features (6169 matrix non-zeros)\n",
      "2016-12-03 23:28:21,570 : INFO : using symmetric alpha at 0.2\n",
      "2016-12-03 23:28:21,571 : INFO : using symmetric eta at 0.2\n",
      "2016-12-03 23:28:21,571 : INFO : using serial LDA version on this node\n",
      "2016-12-03 23:28:21,668 : INFO : running online LDA training, 5 topics, 20 passes over the supplied corpus of 186 documents, updating model once every 186 documents, evaluating perplexity every 186 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2016-12-03 23:28:22,096 : INFO : -32.485 per-word bound, 6011414105.4 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:22,097 : INFO : PROGRESS: pass 0, at document #186/186\n",
      "2016-12-03 23:28:22,249 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"shes\"\n",
      "2016-12-03 23:28:22,251 : INFO : topic #1 (0.200): 0.005*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:22,252 : INFO : topic #2 (0.200): 0.003*\"love\" + 0.003*\"let\" + 0.003*\"baby\" + 0.003*\"girl\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"told\" + 0.002*\"christmas\"\n",
      "2016-12-03 23:28:22,253 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.002*\"tonight\" + 0.002*\"baby\" + 0.002*\"gonna\" + 0.002*\"dont\" + 0.002*\"things\"\n",
      "2016-12-03 23:28:22,253 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"come\"\n",
      "2016-12-03 23:28:22,254 : INFO : topic diff=2.683207, rho=1.000000\n",
      "2016-12-03 23:28:22,529 : INFO : -24.083 per-word bound, 17768302.4 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:22,530 : INFO : PROGRESS: pass 1, at document #186/186\n",
      "2016-12-03 23:28:22,582 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:22,583 : INFO : topic #1 (0.200): 0.005*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:22,584 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"baby\" + 0.003*\"girl\" + 0.002*\"love\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\"\n",
      "2016-12-03 23:28:22,585 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.002*\"tonight\" + 0.002*\"baby\" + 0.002*\"gonna\" + 0.002*\"dont\" + 0.002*\"things\"\n",
      "2016-12-03 23:28:22,586 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"come\"\n",
      "2016-12-03 23:28:22,586 : INFO : topic diff=0.010520, rho=0.577350\n",
      "2016-12-03 23:28:22,888 : INFO : -24.075 per-word bound, 17673650.3 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:22,889 : INFO : PROGRESS: pass 2, at document #186/186\n",
      "2016-12-03 23:28:22,949 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:22,950 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:22,951 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"baby\" + 0.003*\"girl\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"love\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\"\n",
      "2016-12-03 23:28:22,952 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.002*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"dont\" + 0.002*\"come\"\n",
      "2016-12-03 23:28:22,952 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:22,953 : INFO : topic diff=0.004657, rho=0.500000\n",
      "2016-12-03 23:28:23,264 : INFO : -24.073 per-word bound, 17649396.2 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:23,264 : INFO : PROGRESS: pass 3, at document #186/186\n",
      "2016-12-03 23:28:23,317 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:23,318 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:23,318 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"baby\" + 0.003*\"girl\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:23,319 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"dont\" + 0.002*\"come\"\n",
      "2016-12-03 23:28:23,320 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:23,320 : INFO : topic diff=0.002787, rho=0.447214\n",
      "2016-12-03 23:28:23,638 : INFO : -24.072 per-word bound, 17635236.4 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:23,639 : INFO : PROGRESS: pass 4, at document #186/186\n",
      "2016-12-03 23:28:23,698 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:23,699 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:23,699 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.003*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:23,700 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"dont\" + 0.002*\"come\"\n",
      "2016-12-03 23:28:23,701 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:23,702 : INFO : topic diff=0.001837, rho=0.408248\n",
      "2016-12-03 23:28:24,011 : INFO : -24.071 per-word bound, 17623594.8 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:24,011 : INFO : PROGRESS: pass 5, at document #186/186\n",
      "2016-12-03 23:28:24,072 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:24,076 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:24,077 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.003*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:24,078 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"dont\" + 0.002*\"come\"\n",
      "2016-12-03 23:28:24,079 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:24,080 : INFO : topic diff=0.001375, rho=0.377964\n",
      "2016-12-03 23:28:24,413 : INFO : -24.070 per-word bound, 17611498.5 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:24,413 : INFO : PROGRESS: pass 6, at document #186/186\n",
      "2016-12-03 23:28:24,466 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:24,467 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:24,467 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.003*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:24,468 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"dont\" + 0.002*\"come\"\n",
      "2016-12-03 23:28:24,468 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:24,469 : INFO : topic diff=0.000850, rho=0.353553\n",
      "2016-12-03 23:28:24,793 : INFO : -24.070 per-word bound, 17605963.3 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:24,795 : INFO : PROGRESS: pass 7, at document #186/186\n",
      "2016-12-03 23:28:24,850 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:24,851 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:24,852 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.003*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:24,852 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"dont\" + 0.002*\"come\"\n",
      "2016-12-03 23:28:24,853 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:24,854 : INFO : topic diff=0.000550, rho=0.333333\n",
      "2016-12-03 23:28:25,146 : INFO : -24.069 per-word bound, 17603243.6 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:25,146 : INFO : PROGRESS: pass 8, at document #186/186\n",
      "2016-12-03 23:28:25,199 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:25,200 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:25,201 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.003*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:25,202 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"dont\" + 0.002*\"come\"\n",
      "2016-12-03 23:28:25,202 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:25,203 : INFO : topic diff=0.000368, rho=0.316228\n",
      "2016-12-03 23:28:25,472 : INFO : -24.069 per-word bound, 17601859.7 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:25,473 : INFO : PROGRESS: pass 9, at document #186/186\n",
      "2016-12-03 23:28:25,523 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:25,524 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:25,525 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:25,526 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:25,526 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:25,527 : INFO : topic diff=0.000253, rho=0.301511\n",
      "2016-12-03 23:28:25,812 : INFO : -24.069 per-word bound, 17601134.2 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:25,813 : INFO : PROGRESS: pass 10, at document #186/186\n",
      "2016-12-03 23:28:25,891 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"youre\" + 0.002*\"hey\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:25,892 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:25,892 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:25,893 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:25,894 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:25,894 : INFO : topic diff=0.000179, rho=0.288675\n",
      "2016-12-03 23:28:26,201 : INFO : -24.069 per-word bound, 17600737.7 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:26,202 : INFO : PROGRESS: pass 11, at document #186/186\n",
      "2016-12-03 23:28:26,260 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"youre\" + 0.002*\"hey\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:26,261 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:26,262 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:26,262 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:26,263 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:26,264 : INFO : topic diff=0.000130, rho=0.277350\n",
      "2016-12-03 23:28:26,571 : INFO : -24.069 per-word bound, 17600502.3 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:26,572 : INFO : PROGRESS: pass 12, at document #186/186\n",
      "2016-12-03 23:28:26,646 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:26,647 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:26,649 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:26,650 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:26,652 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:26,654 : INFO : topic diff=0.000101, rho=0.267261\n",
      "2016-12-03 23:28:26,929 : INFO : -24.069 per-word bound, 17600328.2 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:26,929 : INFO : PROGRESS: pass 13, at document #186/186\n",
      "2016-12-03 23:28:26,981 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:26,982 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:26,983 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:26,983 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:26,984 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:26,985 : INFO : topic diff=0.000089, rho=0.258199\n",
      "2016-12-03 23:28:27,261 : INFO : -24.069 per-word bound, 17600110.3 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:27,262 : INFO : PROGRESS: pass 14, at document #186/186\n",
      "2016-12-03 23:28:27,316 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:27,316 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:27,317 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:27,318 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:27,319 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:27,319 : INFO : topic diff=0.000102, rho=0.250000\n",
      "2016-12-03 23:28:27,603 : INFO : -24.069 per-word bound, 17598699.6 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:27,603 : INFO : PROGRESS: pass 15, at document #186/186\n",
      "2016-12-03 23:28:27,664 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:27,665 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:27,666 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:27,666 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:27,667 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:27,667 : INFO : topic diff=0.000183, rho=0.242536\n",
      "2016-12-03 23:28:27,969 : INFO : -24.069 per-word bound, 17595763.4 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:27,970 : INFO : PROGRESS: pass 16, at document #186/186\n",
      "2016-12-03 23:28:28,027 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:28,028 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:28,029 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:28,030 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:28,030 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:28,031 : INFO : topic diff=0.000131, rho=0.235702\n",
      "2016-12-03 23:28:28,347 : INFO : -24.069 per-word bound, 17594282.1 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:28,347 : INFO : PROGRESS: pass 17, at document #186/186\n",
      "2016-12-03 23:28:28,416 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:28,417 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:28,418 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:28,419 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:28,420 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:28,422 : INFO : topic diff=0.000096, rho=0.229416\n",
      "2016-12-03 23:28:28,717 : INFO : -24.069 per-word bound, 17593504.5 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:28,718 : INFO : PROGRESS: pass 18, at document #186/186\n",
      "2016-12-03 23:28:28,773 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:28,774 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:28,775 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:28,775 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:28,776 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:28,776 : INFO : topic diff=0.000071, rho=0.223607\n",
      "2016-12-03 23:28:29,053 : INFO : -24.069 per-word bound, 17593074.7 perplexity estimate based on a held-out corpus of 186 documents with 761 words\n",
      "2016-12-03 23:28:29,054 : INFO : PROGRESS: pass 19, at document #186/186\n",
      "2016-12-03 23:28:29,104 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\" + 0.002*\"youve\" + 0.002*\"hey\" + 0.002*\"youre\" + 0.002*\"days\" + 0.002*\"nah\"\n",
      "2016-12-03 23:28:29,105 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\" + 0.003*\"ive\" + 0.003*\"girl\" + 0.002*\"got\" + 0.002*\"easy\" + 0.002*\"youll\"\n",
      "2016-12-03 23:28:29,105 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\" + 0.002*\"la\" + 0.002*\"ill\" + 0.002*\"christmas\" + 0.002*\"told\" + 0.002*\"chains\"\n",
      "2016-12-03 23:28:29,106 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\" + 0.003*\"baby\" + 0.002*\"tonight\" + 0.002*\"gonna\" + 0.002*\"come\" + 0.002*\"dont\"\n",
      "2016-12-03 23:28:29,107 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\" + 0.002*\"ah\" + 0.002*\"id\" + 0.002*\"baby\" + 0.002*\"hey\" + 0.002*\"bye\"\n",
      "2016-12-03 23:28:29,107 : INFO : topic diff=0.000054, rho=0.218218\n",
      "2016-12-03 23:28:29,108 : INFO : topic #0 (0.200): 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\"\n",
      "2016-12-03 23:28:29,109 : INFO : topic #1 (0.200): 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\"\n",
      "2016-12-03 23:28:29,110 : INFO : topic #2 (0.200): 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\"\n",
      "2016-12-03 23:28:29,110 : INFO : topic #3 (0.200): 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\"\n",
      "2016-12-03 23:28:29,111 : INFO : topic #4 (0.200): 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.004*\"john\" + 0.003*\"love\" + 0.003*\"brian\" + 0.002*\"time\" + 0.002*\"mm\"\n",
      "Topic 1 : 0.006*\"love\" + 0.004*\"want\" + 0.003*\"yeah\" + 0.003*\"oh\" + 0.003*\"ill\"\n",
      "Topic 2 : 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"baby\" + 0.002*\"ooh\" + 0.002*\"better\"\n",
      "Topic 3 : 0.004*\"long\" + 0.003*\"love\" + 0.003*\"yeah\" + 0.003*\"im\" + 0.003*\"know\"\n",
      "Topic 4 : 0.003*\"needed\" + 0.002*\"im\" + 0.002*\"sun\" + 0.002*\"dont\" + 0.002*\"forget\"\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "lda_model = cread_lda(dictionary, corpus, num_topics, )\n",
    "for t, top_words in lda_model.print_topics(num_topics=num_topics, num_words=5):\n",
    "    print(\"Topic\", t, \":\", top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1822! [(0, 0.95238184057863251), (1, 0.011806327241384274), (2, 0.011806038336300608), (3, 0.01205437286871662), (4, 0.011951420974965923)]\n",
      "A Day In The Life [(0, 0.99166360790163754)]\n",
      "A Hard Day's Night [(0, 0.87334633608717138), (3, 0.12036482836703136)]\n",
      "A Little Rhyme [(0, 0.11937755548646665), (3, 0.86702457084998119)]\n",
      "A Shot Of Rhythm And Blues [(3, 0.99359494701706841)]\n",
      "A Taste Of Honey [(3, 0.98196429248794947)]\n",
      "Across The Universe [(3, 0.99397226067866862)]\n",
      "Act Naturally [(3, 0.99075636526347888)]\n",
      "Ain't She Sweet [(1, 0.99111079096012977)]\n",
      "All I've Got To Do [(3, 0.98369702398346603)]\n",
      "All My Loving [(1, 0.033794861305270964), (2, 0.84789377810861766), (3, 0.11284831260718597)]\n",
      "All Things Must Pass [(3, 0.98691002643177894)]\n",
      "All Together Now [(3, 0.98722457253916729)]\n",
      "All You Need Is Love [(1, 0.99156018291903891)]\n",
      "And I Love Her [(3, 0.97920966667464626)]\n",
      "And Your Bird Can Sing [(4, 0.98014722887354611)]\n",
      "Anna, Go To Him [(1, 0.96877728216327685)]\n",
      "Another Girl [(1, 0.98868445147294848)]\n",
      "Any Time At All [(0, 0.69691895176148988), (3, 0.29170826605306494)]\n",
      "Ask Me Why [(1, 0.90714060374973571), (3, 0.081944800581354074)]\n"
     ]
    }
   ],
   "source": [
    "# Получение темы для конкретного документа\n",
    "for i in range(20):\n",
    "    print(song_names[i], lda_model[corpus[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other test. Nightwish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 23:29:01,602 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2016-12-03 23:29:01,614 : INFO : built Dictionary(1991 unique tokens: ['moonlit', 'believe', 'bride', 'called', 'eternal']...) from 65 documents (total 5430 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "folder = './../data/nightwish/'\n",
    "song_names, raw_texts = read_data(folder)\n",
    "texts = prepare_data(raw_texts)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 23:29:03,909 : INFO : collecting document frequencies\n",
      "2016-12-03 23:29:03,910 : INFO : PROGRESS: processing document #0\n",
      "2016-12-03 23:29:03,911 : INFO : calculating IDF weights for 65 documents and 1990 features (3833 matrix non-zeros)\n",
      "2016-12-03 23:29:03,913 : INFO : using serial LSI version on this node\n",
      "2016-12-03 23:29:03,914 : INFO : updating model with new documents\n",
      "2016-12-03 23:29:03,919 : INFO : preparing a new chunk of documents\n",
      "2016-12-03 23:29:03,921 : INFO : using 100 extra samples and 2 power iterations\n",
      "2016-12-03 23:29:03,921 : INFO : 1st phase: constructing (1991, 105) action matrix\n",
      "2016-12-03 23:29:03,923 : INFO : orthonormalizing (1991, 105) action matrix\n",
      "2016-12-03 23:29:03,974 : INFO : 2nd phase: running dense svd on (105, 65) matrix\n",
      "2016-12-03 23:29:03,986 : INFO : computing the final decomposition\n",
      "2016-12-03 23:29:03,986 : INFO : keeping 5 factors (discarding 87.310% of energy spectrum)\n",
      "2016-12-03 23:29:03,993 : INFO : processed documents up to #65\n",
      "2016-12-03 23:29:03,995 : INFO : topic #0(1.600): 0.196*\"wish\" + 0.132*\"love\" + 0.125*\"heart\" + 0.121*\"world\" + 0.110*\"time\" + 0.108*\"come\" + 0.107*\"oh\" + 0.103*\"night\" + 0.100*\"long\" + 0.094*\"eyes\"\n",
      "2016-12-03 23:29:03,997 : INFO : topic #1(1.145): -0.381*\"wish\" + -0.238*\"angel\" + -0.167*\"miss\" + 0.134*\"drown\" + -0.131*\"im\" + -0.124*\"getting\" + -0.124*\"colder\" + -0.112*\"right\" + 0.108*\"ago\" + -0.107*\"wings\"\n",
      "2016-12-03 23:29:03,999 : INFO : topic #2(1.127): 0.181*\"walks\" + 0.172*\"forever\" + 0.171*\"heaven\" + 0.160*\"heart\" + -0.147*\"right\" + 0.144*\"walk\" + 0.125*\"meadows\" + -0.125*\"crestfallen\" + 0.117*\"little\" + 0.109*\"wish\"\n",
      "2016-12-03 23:29:04,000 : INFO : topic #3(1.118): 0.219*\"heaven\" + 0.173*\"meadows\" + -0.148*\"horizon\" + -0.130*\"world\" + 0.127*\"cradle\" + -0.126*\"away\" + -0.124*\"end\" + -0.121*\"far\" + -0.111*\"ago\" + -0.109*\"long\"\n",
      "2016-12-03 23:29:04,003 : INFO : topic #4(1.106): 0.176*\"right\" + 0.162*\"night\" + 0.159*\"things\" + -0.156*\"wheres\" + 0.153*\"fight\" + 0.118*\"land\" + 0.116*\"horizon\" + 0.115*\"lost\" + 0.109*\"crestfallen\" + -0.107*\"dont\"\n",
      "2016-12-03 23:29:04,005 : INFO : topic #0(1.600): 0.196*\"wish\" + 0.132*\"love\" + 0.125*\"heart\" + 0.121*\"world\" + 0.110*\"time\" + 0.108*\"come\" + 0.107*\"oh\" + 0.103*\"night\" + 0.100*\"long\" + 0.094*\"eyes\"\n",
      "2016-12-03 23:29:04,006 : INFO : topic #1(1.145): -0.381*\"wish\" + -0.238*\"angel\" + -0.167*\"miss\" + 0.134*\"drown\" + -0.131*\"im\" + -0.124*\"getting\" + -0.124*\"colder\" + -0.112*\"right\" + 0.108*\"ago\" + -0.107*\"wings\"\n",
      "2016-12-03 23:29:04,007 : INFO : topic #2(1.127): 0.181*\"walks\" + 0.172*\"forever\" + 0.171*\"heaven\" + 0.160*\"heart\" + -0.147*\"right\" + 0.144*\"walk\" + 0.125*\"meadows\" + -0.125*\"crestfallen\" + 0.117*\"little\" + 0.109*\"wish\"\n",
      "2016-12-03 23:29:04,008 : INFO : topic #3(1.118): 0.219*\"heaven\" + 0.173*\"meadows\" + -0.148*\"horizon\" + -0.130*\"world\" + 0.127*\"cradle\" + -0.126*\"away\" + -0.124*\"end\" + -0.121*\"far\" + -0.111*\"ago\" + -0.109*\"long\"\n",
      "2016-12-03 23:29:04,009 : INFO : topic #4(1.106): 0.176*\"right\" + 0.162*\"night\" + 0.159*\"things\" + -0.156*\"wheres\" + 0.153*\"fight\" + 0.118*\"land\" + 0.116*\"horizon\" + 0.115*\"lost\" + 0.109*\"crestfallen\" + -0.107*\"dont\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.196*\"wish\" + 0.132*\"love\" + 0.125*\"heart\" + 0.121*\"world\" + 0.110*\"time\" + 0.108*\"come\" + 0.107*\"oh\" + 0.103*\"night\" + 0.100*\"long\" + 0.094*\"eyes\"\n",
      "Topic 1 : -0.381*\"wish\" + -0.238*\"angel\" + -0.167*\"miss\" + 0.134*\"drown\" + -0.131*\"im\" + -0.124*\"getting\" + -0.124*\"colder\" + -0.112*\"right\" + 0.108*\"ago\" + -0.107*\"wings\"\n",
      "Topic 2 : 0.181*\"walks\" + 0.172*\"forever\" + 0.171*\"heaven\" + 0.160*\"heart\" + -0.147*\"right\" + 0.144*\"walk\" + 0.125*\"meadows\" + -0.125*\"crestfallen\" + 0.117*\"little\" + 0.109*\"wish\"\n",
      "Topic 3 : 0.219*\"heaven\" + 0.173*\"meadows\" + -0.148*\"horizon\" + -0.130*\"world\" + 0.127*\"cradle\" + -0.126*\"away\" + -0.124*\"end\" + -0.121*\"far\" + -0.111*\"ago\" + -0.109*\"long\"\n",
      "Topic 4 : 0.176*\"right\" + 0.162*\"night\" + 0.159*\"things\" + -0.156*\"wheres\" + 0.153*\"fight\" + 0.118*\"land\" + 0.116*\"horizon\" + 0.115*\"lost\" + 0.109*\"crestfallen\" + -0.107*\"dont\"\n"
     ]
    }
   ],
   "source": [
    "lsi_model = create_lsi(dictionary, corpus)\n",
    "for t, topic in lsi_model.print_topics(num_topics):\n",
    "    print('Topic {0} : {1}'.format(t, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-03 23:29:06,720 : INFO : collecting document frequencies\n",
      "2016-12-03 23:29:06,721 : INFO : PROGRESS: processing document #0\n",
      "2016-12-03 23:29:06,722 : INFO : calculating IDF weights for 65 documents and 1990 features (3833 matrix non-zeros)\n",
      "2016-12-03 23:29:06,724 : INFO : using symmetric alpha at 0.2\n",
      "2016-12-03 23:29:06,724 : INFO : using symmetric eta at 0.2\n",
      "2016-12-03 23:29:06,725 : INFO : using serial LDA version on this node\n",
      "2016-12-03 23:29:06,774 : INFO : running online LDA training, 5 topics, 20 passes over the supplied corpus of 65 documents, updating model once every 65 documents, evaluating perplexity every 65 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2016-12-03 23:29:07,042 : INFO : -50.356 per-word bound, 1441357516219480.0 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:07,042 : INFO : PROGRESS: pass 0, at document #65/65\n",
      "2016-12-03 23:29:07,109 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:07,110 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"im\" + 0.002*\"heaven\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"like\" + 0.001*\"blue\" + 0.001*\"miss\" + 0.001*\"sky\"\n",
      "2016-12-03 23:29:07,111 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.002*\"shall\" + 0.001*\"memories\" + 0.001*\"heart\" + 0.001*\"forever\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:07,112 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:07,113 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.002*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:07,113 : INFO : topic diff=2.678972, rho=1.000000\n",
      "2016-12-03 23:29:07,266 : INFO : -35.242 per-word bound, 40645456414.0 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:07,267 : INFO : PROGRESS: pass 1, at document #65/65\n",
      "2016-12-03 23:29:07,290 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:07,291 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"wish\" + 0.001*\"like\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:07,292 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"heart\" + 0.001*\"forever\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:07,293 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:07,293 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:07,294 : INFO : topic diff=0.011715, rho=0.577350\n",
      "2016-12-03 23:29:07,446 : INFO : -35.238 per-word bound, 40514392888.1 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:07,447 : INFO : PROGRESS: pass 2, at document #65/65\n",
      "2016-12-03 23:29:07,465 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:07,467 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"wish\" + 0.002*\"like\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:07,468 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"heart\" + 0.001*\"forever\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:07,468 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:07,469 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:07,470 : INFO : topic diff=0.004441, rho=0.500000\n",
      "2016-12-03 23:29:07,654 : INFO : -35.237 per-word bound, 40491619479.8 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:07,654 : INFO : PROGRESS: pass 3, at document #65/65\n",
      "2016-12-03 23:29:07,677 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:07,679 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:07,680 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"heart\" + 0.001*\"forever\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:07,681 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:07,683 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:07,684 : INFO : topic diff=0.002022, rho=0.447214\n",
      "2016-12-03 23:29:07,857 : INFO : -35.237 per-word bound, 40486172579.8 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:07,857 : INFO : PROGRESS: pass 4, at document #65/65\n",
      "2016-12-03 23:29:07,879 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:07,880 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:07,882 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"heart\" + 0.001*\"forever\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:07,883 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:07,884 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:07,885 : INFO : topic diff=0.001030, rho=0.408248\n",
      "2016-12-03 23:29:08,072 : INFO : -35.237 per-word bound, 40484587126.9 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:08,073 : INFO : PROGRESS: pass 5, at document #65/65\n",
      "2016-12-03 23:29:08,091 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:08,093 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:08,093 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:08,094 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:08,095 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:08,095 : INFO : topic diff=0.000568, rho=0.377964\n",
      "2016-12-03 23:29:08,253 : INFO : -35.237 per-word bound, 40484058160.5 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:08,253 : INFO : PROGRESS: pass 6, at document #65/65\n",
      "2016-12-03 23:29:08,276 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:08,277 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:08,279 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:08,284 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:08,286 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:08,287 : INFO : topic diff=0.000332, rho=0.353553\n",
      "2016-12-03 23:29:08,459 : INFO : -35.237 per-word bound, 40483862361.6 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:08,459 : INFO : PROGRESS: pass 7, at document #65/65\n",
      "2016-12-03 23:29:08,477 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:08,478 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:08,479 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:08,479 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:08,480 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:08,481 : INFO : topic diff=0.000203, rho=0.333333\n",
      "2016-12-03 23:29:08,641 : INFO : -35.237 per-word bound, 40483784077.3 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:08,642 : INFO : PROGRESS: pass 8, at document #65/65\n",
      "2016-12-03 23:29:08,660 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:08,661 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:08,662 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:08,663 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:08,663 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:08,664 : INFO : topic diff=0.000128, rho=0.316228\n",
      "2016-12-03 23:29:08,832 : INFO : -35.237 per-word bound, 40483750446.8 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:08,832 : INFO : PROGRESS: pass 9, at document #65/65\n",
      "2016-12-03 23:29:08,851 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:08,851 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:08,852 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:08,853 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:08,853 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:08,854 : INFO : topic diff=0.000084, rho=0.301511\n",
      "2016-12-03 23:29:09,008 : INFO : -35.237 per-word bound, 40483735413.0 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:09,010 : INFO : PROGRESS: pass 10, at document #65/65\n",
      "2016-12-03 23:29:09,036 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:09,040 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:09,042 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:09,043 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:09,044 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:09,044 : INFO : topic diff=0.000056, rho=0.288675\n",
      "2016-12-03 23:29:09,200 : INFO : -35.237 per-word bound, 40483728496.3 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:09,201 : INFO : PROGRESS: pass 11, at document #65/65\n",
      "2016-12-03 23:29:09,220 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:09,221 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:09,221 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:09,222 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:09,223 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:09,223 : INFO : topic diff=0.000038, rho=0.277350\n",
      "2016-12-03 23:29:09,382 : INFO : -35.237 per-word bound, 40483725042.7 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:09,382 : INFO : PROGRESS: pass 12, at document #65/65\n",
      "2016-12-03 23:29:09,401 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:09,402 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:09,403 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:09,403 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:09,404 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:09,405 : INFO : topic diff=0.000027, rho=0.267261\n",
      "2016-12-03 23:29:09,563 : INFO : -35.237 per-word bound, 40483723253.6 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:09,564 : INFO : PROGRESS: pass 13, at document #65/65\n",
      "2016-12-03 23:29:09,582 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:09,583 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:09,583 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:09,584 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:09,585 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:09,585 : INFO : topic diff=0.000019, rho=0.258199\n",
      "2016-12-03 23:29:09,743 : INFO : -35.237 per-word bound, 40483722147.6 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:09,743 : INFO : PROGRESS: pass 14, at document #65/65\n",
      "2016-12-03 23:29:09,762 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:09,763 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:09,764 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:09,764 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:09,765 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:09,766 : INFO : topic diff=0.000014, rho=0.250000\n",
      "2016-12-03 23:29:09,924 : INFO : -35.237 per-word bound, 40483721803.7 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:09,925 : INFO : PROGRESS: pass 15, at document #65/65\n",
      "2016-12-03 23:29:09,944 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:09,944 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:09,945 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:09,946 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:09,946 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:09,947 : INFO : topic diff=0.000010, rho=0.242536\n",
      "2016-12-03 23:29:10,105 : INFO : -35.237 per-word bound, 40483721309.8 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:10,106 : INFO : PROGRESS: pass 16, at document #65/65\n",
      "2016-12-03 23:29:10,125 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:10,126 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:10,126 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:10,127 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:10,128 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:10,128 : INFO : topic diff=0.000007, rho=0.235702\n",
      "2016-12-03 23:29:10,279 : INFO : -35.237 per-word bound, 40483721224.4 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:10,281 : INFO : PROGRESS: pass 17, at document #65/65\n",
      "2016-12-03 23:29:10,302 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:10,303 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:10,304 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:10,305 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:10,305 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:10,306 : INFO : topic diff=0.000005, rho=0.229416\n",
      "2016-12-03 23:29:10,459 : INFO : -35.237 per-word bound, 40483721121.2 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:10,459 : INFO : PROGRESS: pass 18, at document #65/65\n",
      "2016-12-03 23:29:10,478 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:10,479 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:10,482 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:10,483 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:10,484 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:10,485 : INFO : topic diff=0.000004, rho=0.223607\n",
      "2016-12-03 23:29:10,644 : INFO : -35.237 per-word bound, 40483721082.6 perplexity estimate based on a held-out corpus of 65 documents with 398 words\n",
      "2016-12-03 23:29:10,644 : INFO : PROGRESS: pass 19, at document #65/65\n",
      "2016-12-03 23:29:10,662 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\" + 0.001*\"philosopher\" + 0.001*\"sun\" + 0.001*\"nights\" + 0.001*\"master\" + 0.001*\"beautiful\"\n",
      "2016-12-03 23:29:10,663 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\" + 0.002*\"wish\" + 0.001*\"praise\" + 0.001*\"blue\" + 0.001*\"sky\" + 0.001*\"miss\"\n",
      "2016-12-03 23:29:10,664 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\" + 0.001*\"forever\" + 0.001*\"heart\" + 0.001*\"love\" + 0.001*\"eva\" + 0.001*\"resting\"\n",
      "2016-12-03 23:29:10,665 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\" + 0.002*\"opera\" + 0.001*\"rain\" + 0.001*\"drown\" + 0.001*\"come\" + 0.001*\"wish\"\n",
      "2016-12-03 23:29:10,666 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\" + 0.001*\"got\" + 0.001*\"night\" + 0.001*\"instead\" + 0.001*\"elvenpath\" + 0.001*\"dont\"\n",
      "2016-12-03 23:29:10,666 : INFO : topic diff=0.000003, rho=0.218218\n",
      "2016-12-03 23:29:10,668 : INFO : topic #0 (0.200): 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\"\n",
      "2016-12-03 23:29:10,669 : INFO : topic #1 (0.200): 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\"\n",
      "2016-12-03 23:29:10,670 : INFO : topic #2 (0.200): 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\"\n",
      "2016-12-03 23:29:10,671 : INFO : topic #3 (0.200): 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\"\n",
      "2016-12-03 23:29:10,671 : INFO : topic #4 (0.200): 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.002*\"angel\" + 0.002*\"wish\" + 0.001*\"nighttime\" + 0.001*\"queen\" + 0.001*\"1001\"\n",
      "Topic 1 : 0.002*\"slow\" + 0.002*\"meadows\" + 0.002*\"heaven\" + 0.002*\"im\" + 0.002*\"like\"\n",
      "Topic 2 : 0.002*\"little\" + 0.002*\"shall\" + 0.002*\"walks\" + 0.002*\"leave\" + 0.001*\"memories\"\n",
      "Topic 3 : 0.002*\"bye\" + 0.002*\"world\" + 0.002*\"end\" + 0.002*\"long\" + 0.002*\"phantom\"\n",
      "Topic 4 : 0.002*\"wheres\" + 0.002*\"lost\" + 0.002*\"killing\" + 0.002*\"complete\" + 0.001*\"things\"\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "lda_model = cread_lda(dictionary, corpus, num_topics)\n",
    "for t, top_words in lda_model.print_topics(num_topics=num_topics, num_words=5):\n",
    "    print(\"Topic\", t, \":\", top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplic 0 : angel wish nighttime queen 1001 philosopher sun nights master beautiful\n",
      "\n",
      "Toplic 1 : slow meadows heaven im like wish praise blue sky miss\n",
      "\n",
      "Toplic 2 : little shall walks leave memories forever heart love eva resting\n",
      "\n",
      "Toplic 3 : bye world end long phantom opera rain drown come wish\n",
      "\n",
      "Toplic 4 : wheres lost killing complete things got night instead elvenpath dont\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic, words in enumerate(get_top_words(lda_model)):\n",
    "    print('Toplic {0} : {1}\\n'.format(topic, ' '.join(words[:10])))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
